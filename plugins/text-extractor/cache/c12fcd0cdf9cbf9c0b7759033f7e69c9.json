{"path":".obsidian/plugins/text-extractor/cache/an/profane-世俗-论文-英文-AnalysisoftheuseofdiscretewavelettransformscoupledwithANNforshort-termstreamflowforecasting.pdf-c12fcd0cdf9cbf9c0b7759033f7e69c9.json","text":"Applied Soft Computing Journal 80 (2019) 494–505 Contents lists available at ScienceDirect Applied Soft Computing Journal journal homepage: www.elsevier.com/locate/asoc Analysis of the use of discrete wavelet transforms coupled with ANN for short-term streamflow forecasting Paula Karenina de Macedo Machado Freire, Celso Augusto Guimarães Santos ∗ , Gustavo Barbosa Lima da Silva Department of Civil and Environmental Engineering, Federal University of Paraíba, 58051-900 João Pessoa, Paraíba, Brazil h i g h l i g h t s • Fifty-four mother-wavelets were as- sessed. • The high-frequency components were considered noise and then elimi- nated. • Thirty-four combinations of low-frequency components were used as ANN in- puts. • A total of 1836 wavelet-ANN hybrid models (54 × 34) were tested. • Discrete Meyer mother-wavelet with the approximation A3 showed the best results. g r a p h i c a l a b s t r a c t a r t i c l e i n f o Article history: Received 9 November 2018 Received in revised form 2 April 2019 Accepted 17 April 2019 Available online 24 April 2019 Keywords: Artificial neural network DWT Inflow prediction Sobradinho reservoir a b s t r a c t The use of wavelet transforms to forecast daily streamflows into the Sobradinho Reservoir (Bahia State, Brazil) seven days ahead by a wavelet-artificial neural network (ANN) hybrid system was analyzed in this paper. This work also determined the appropriate mother-wavelet for this type of forecasting with an ANN, performed 1836 simulations with the wavelet-ANN hybrid systems (tested with 54 mother- wavelets) and compared the results with the predictions made without the application of a wavelet transform (henceforth called a stand-alone ANN). Daily data from January 1931 to December 2010 were used. According to the results, the wavelet-ANN hybrid system performed better than the system using the ANN with the raw data. The approximation A3 from the discrete Meyer mother-wavelet obtained the best results; the root mean square error (RMSE) decreased by approximately 80%, while the R 2 and NASH coefficients increased by more than 5% and 10%, respectively, compared with the stand-alone ANN. © 2019 Elsevier B.V. All rights reserved. 1. Introduction Daily streamflow forecasting is one of the most important research interests in hydrology and is used for the planning and management of water systems; these interests include reservoir ∗ Corresponding author. E-mail address: celso@ct.ufpb.br (C.A.G. Santos). operation, dam construction, wastewater disposal, and flood con- trol [1]. Several researchers have studied this issue over the last few decades. In Brazil, the National Electrical System Operator (ONS) is the body responsible for preparing forecasts and gener- ating scenarios of the natural mean daily, weekly and monthly streamflows for all hydroelectric power stations in the coun- try. ONS currently uses the stochastic model PREVIVAZ [2] to perform these forecasts, which is based on a set of stochastic https://doi.org/10.1016/j.asoc.2019.04.024 1568-4946/© 2019 Elsevier B.V. All rights reserved. P.K.M.M. Freire, C.A.G. Santos and G.B.L. Silva / Applied Soft Computing Journal 80 (2019) 494–505 495 autoregressive (AR) and autoregressive moving average (ARMA) models. Although the application of AR models is common in flow modeling studies due to their simplicity and leniency, researchers have reported that AR models may not be applied appropriately to represent the complex relationships between streamflow and previous values [3,4]. This result is because AR models assume linear relationships among variables, which are not appropriate for forecasting highly nonlinear processes, such as hydrological processes. The number of studies that have used models based on artifi- cial neural networks (ANNs) has increased significantly in recent years to overcome the shortcomings of the current mathematical models used to forecast streamflows (e.g., [1,5–13]). The advan- tage of using ANNs in various fields of science, particularly in time series studies, is due to its proven ability to adequately represent such highly nonlinear relations among variables. By means of this technique, the dependence between past and future occurrences of a time series can be modeled by a combination of many nonlinear mathematical functions, which can lead to a representation of complex systems with an acceptable precision. Thus, because ANNs can overcome many difficulties presented in linear models, such as autoregressive integrated moving av- erage and traditional nonlinear regression techniques, they have been proven to be a useful tool for hydrological prediction and have often been adopted by researchers in the field of water resources [14]. Lauzon et al. [8] mentioned that the more flexible structure of the ANNs allows a best reproduction of complex hy- drological processes. However, the ANNs are more sensitive to the noises present in observed natural time series, and Cigizoglu [12] also noted that it is apparent that an ANN can provide a tighter fit to the data than conventional AR models. Although good results were obtained when ANNs were used to model hydrologic time series, some difficulties are still being noted when applying this technique to streamflow forecasting; thus, improvement in the result accuracy is still desired. Wu et al. [15] observed that the ANN is a parallel-distributed informa- tion processing system with an excellent nonlinearity capturing ability, but the noise that is usually present in a hydrologic time series should influence the forecasting quality and also that the high autocorrelation between past and future values of the streamflow data often introduce the lagged predictions for the ANN model. To address these issues, the methods that are used to pre- process the input data have been noted as efficient alternatives for improving ANN model performance. Wavelet analysis could be cited as an example of such a preprocessing procedure, as found in Wang & Ding [16], who concluded that the preprocessed input data could increase the forecasted accuracy and prolong the length time of prediction. Similar conclusions are found in Cannas et al. [17] and Nourani et al. [18]. Therefore, wavelet analysis has recently received considerable attention and has been often used to remove noise, as noted by Johnstone & Sil- verman [19], Dahdouh et al. [20] and San Emeterio & Rodriguez- Hernandez [21]. Cannas et al. [22] utilized an ANN to predict the monthly streamflows of an Italian river one month ahead using a discrete and continuous wavelet to preprocess the inputs and outputs. Kisi [23] used a wavelet transform for the data preprocessing for a linear regression model and showed better results than forecasts using only ANNs. Maheswaran & Khosa [24] developed a hybrid model consisting of a wavelet transform and second-order Volterra kernels that used a formulation of the Kalman filter to forecast streamflows for the next month. To date, most of the published applications involving wavelet- ANN models for short-term streamflow forecasting used the de- composed signals, i.e., the low-frequency components (approxi- mations) and high-frequency components (details) for a certain decomposition level, that were extracted from the wavelet anal- ysis; e.g., see [15,25–30]. These details can be considered noise, as shown in [31], who used only approximations as model inputs for an ANN, and in [32], who used the approximations as well as their linear combinations (i.e., approximation summation) as model inputs. Both studies obtained excellent results. Although Santos & Silva [32] showed that such linear combinations of the approximations could improve the forecasting results, they performed the simulations using only one wavelet family and one subfamily (i.e., Daubechies, db10). Several authors found difficulty in forecasting accurate daily streamflows with long lead times, i.e., seven days [33]. Thus, this paper seeks to improve this performance by using the hybrid system proposed by Santos & Silva [32]. However, this paper tests several wavelet families (in addition to the Daubechies, e.g., Sym- lets, coiflets, discrete Meyer) and subfamilies with linear combi- nations of approximations as model inputs to assess the efficiency of wavelet-ANN models. To assess the efficiency, the model will be used for the seven-day forecasting of daily streamflows into the Sobradinho Reservoir in northeastern Brazil, and the results will be compared with those from models based on stand-alone ANNs (without the application of a wavelet transform). 2. Materials and methods 2.1. Wavelet transform Mallat [34] developed an efficient method using filters to calculate wavelet coefficients at each position based on powers of two, called the dyadic scales; this analysis is performed through a discrete wavelet transform (DWT). The Mallat algorithm is a scheme called a two-channel sub-band coder, which is, in fact, a classical scheme used by the signal processing community. 2.1.1. Decomposition In the filtering process, the original signal (Q ) is decomposed into two signals called details (D) and approximations (A) through high-pass and low-pass filters, respectively, i.e., Q = A1 + D1. On the other hand, the obtained approximation (A1) can be again decomposed into lower approximation (A2) and detail (D2), then A1 = A2 + D2. This means that Q = A2 + D2 + D1, and so on. Thus, in a general form Q = An + ∑n i=1 Di. For a better understanding, the details are the low-scale, high-frequency com- ponents, whereas the approximations are the high-scale, low- frequency components of the signal. Unfortunately, such an op- eration produces twice as much data as the original time series. However, only one point out of two in each of the filter results can be preserved to obtain complete information. This is known as down sampling, which produces the sequences cA and cD with the so-called DWT coefficients (c). This filtering process results in the detail and approximation coefficient vectors with approximately half the length of the original time series. This decomposition process can be iterated, and the successive ap- proximations can be decomposed again; then, the original signal can be broken down into several lower-resolution components via an infinite process. However, based on the nature of the stud- ied signal, the original signal is decomposed up to ten levels in the present study. Fig. 1 shows an example of the decomposition (at levels 1, 4, 7 and 10) of the original streamflow time series into approximations and details. 2.1.2. Family wavelet Several wavelet basis functions can be used in such a trans- formation, and the basic wavelet selection may affect the final results. Thus, in this paper, the Haar, Daubechies (db), biorthog- onal (bior), reverse biorthogonal (rbior), coiflets (coif), Symlets (sym) and discrete Meyer (dmey) wavelets were tested, which are shown in Fig. 2. 496 P.K.M.M. Freire, C.A.G. Santos and G.B.L. Silva / Applied Soft Computing Journal 80 (2019) 494–505 Fig. 1. Example of the decomposition of a streamflow time series into approximations (A) and details (D) at levels 1, 4, 7 and 10. Fig. 2. The Haar, Daubechies (db), biorthogonal (bior), reverse biorthogonal (rbior), coiflets (coif), Symlets (sym) and discrete Meyer (dmey) wavelet families. 2.2. Artificial neural network ANNs are inspired by the biological neural system and learn through example similar to the human brain and store the ac- quired knowledge in the connection weights between neurons [35]. An ANN can be defined as a set of simple processing units called neurons that work as a parallel distributed processor and are responsible for storing experimental knowledge for later use [36]. In this paper, the ANN inputs were composed of the stream- flows observed on the current day t (Qt ) and the previous four days (Qt−1, Qt−2, Qt−3 and Qt−4), based on the performance of 15 000 runs. The input layer thus consisted of five neurons. When the ANN was coupled with the wavelet transform, the input was formed by the signal approximation (e.g., A1) or their lin- ear combination (e.g., A1 + A2 + A3). This type of combination amplifies the ANN inputs but considers different levels of signal smoothness, which helps the system to identify and quantify P.K.M.M. Freire, C.A.G. Santos and G.B.L. Silva / Applied Soft Computing Journal 80 (2019) 494–505 497 the parameters affecting the behavior of the system. The output layer had only one neuron, which corresponds to the forecasted streamflow seven days later (Qt+7), as shown in Fig. 3 for the stand-alone ANN. The ANN application essentially consists of three stages: (a) architecture, (b) learning, and (c) verification. 2.2.1. Architecture The architecture of an ANN (Fig. 4) consists of the determined number of layers, the neurons per layer, the type of connec- tion between the layers (activation functions) and the network topology. Several ANN configurations can be tested; however, the degrees of freedom would be very high, which means that it would be unclear whether a better performance was due to the proposed wavelet-ANN hybrid system or the ANN configuration. Thus, first the number of neurons was varied from 1 to 10 in the input layer and from 1 to 30 in the hidden layer (10 × 30 = 300 runs), and to analyze the parameter sensitivity on the results, 50 independent runs were performed (giving a total of 50 × 300 = 15 000 runs). Based on those 15 000 tests, it was observed that more than five neurons in the input layer and more than 20 neurons in the hidden layer, the RMSE of the training set was not found to improve significantly. Then, this architecture was chosen because it produced the best result in terms of RMSE. A feed-forward network with two layers was proposed for this study as shown in Fig. 4. Layer one has 20 hidden neurons (defined by means of the 15 000 independent runs) and a sigmoid activation function, which is considered a good activation function due to its generally accepted behavior. Layer two has one output neuron and a linear activation function. These layers make multidimensional mapping fairly straightfor- ward by providing consistent data with sufficient neurons in the hidden layer. After that, with the ANN configuration fixed, 1836 models (54 wavelet subfamilies × 34 linear combinations of the approximations) were analyzed. 2.2.2. Learning The learning process consists of obtaining a set of optimal weights (model parameters) that will allow the model to produce the best representation of the input–output relationship. Learning is an iterative process in which the weights are updated to mini- mize the error between the network and target outputs to archive the optimal generalization of the model. This result is accom- plished by providing the network with several training samples, each consisting of a specific input pattern and corresponding output response (correct value). The updating of the synaptic weights is proportional to the difference between the actual output and the output calculated by the ANN, which is directly proportional to the weights used. Several algorithms called learning rules are used to update the weights. In the present work, the Levenberg–Marquardt algo- rithm was used because it is one of the fastest and most efficient methods for training ANNs [37,38]. 2.2.3. Verification The ANN was verified by calculating the mean square error (MSE): MSE = 1 N N∑ t=1 (Qct − Qot )2 (1) where N is the number of data, and Qct and Qot are the calculated and observed streamflow, respectively, at time t. Table 1 Descriptive statistics for the observed daily streamflow into the Sobradinho Reservoir (1931 to 2010). Statistics Value Average 2 655.61 m3/s Mode 1 054.00 m3/s Median 1 868.00 m3/s Harmonic mean 1 664.23 m3/s Geometric mean 2 068.38 m3/s Sum 77 597 049.02 m3/s Minimum 400.00 m3/s Maximum 18 525.00 m3/s Range 18 125.00 m3/s First quartile 1 163.00 m3/s Third quartile 3 647.25 m3/s Interquartile range 2 484.25 m3/s Variance 4 164 448.54 (m3/s) 2 Standard deviation 2 040.70 m3/s Coefficient of variation 0.77 – Skewness 1.78 – Kurtosis 4.98 – Excess kurtosis 1.98 – Percentiles 0.01% 428.44 m3/s 0.10% 481.44 m3/s 0.90% 611.97 m3/s 0.99% 617.26 m3/s 25% 1 163.00 m3/s 50% 1 868.00 m3/s 75% 3 647.25 m3/s 2.3. Field data The studied time series is the natural daily inflows to the So- bradinho Reservoir, whose drainage basin has an area of 498 968 km 2 (Fig. 5), which is larger than Sweden, for example. The streamflow time series had 29 220 records, corresponding to 1 January 1931 to 31 December 2010, that were obtained from the ONS at www.ons.org.br, which is an institution responsible for the operation of the Brazilian interlinked electric system. The daily hydrograph of the streamflow into the Sobradinho Reservoir (1931−2010) is also shown in Fig. 1, with an average streamflow of 2655.61 m 3/s, a maximum of 18 525 m 3/s and a minimum of 400 m3/s. These values and other statistical indices can be seen in Table 1. 2.4. Performance evaluation The applied methodology (Fig. 6) consisted of the following steps. (a) Perform the seven-day forecasting with the stand-alone ANN using the raw signal set (streamflow of today and stream- flow of four previous days) and calculate the performance indices. (b) Select a mother-wavelet to be used in the decomposition of the raw signal. (c) Based on the size of the raw signal time series and the selected mother-wavelet, calculate the maximum decomposition level for each mother-wavelet. (d) Set the optimal level of decomposition to nopt. (e) Decompose the signal up to the optimal decomposition level (A1, A2, . . . , Anopt ). (f) Determine the approximation and linear combinations of four sequential approximations (An, An + An+1, An + An+1 + An+2, An + An+1 + An+2 + An+3, An+1, An+1 + An+2, An+1 + An+2 + An+3, An+1 + An+2 + An+3 + An+4, . . . , Anopt−1 + Anopt , Anopt ). (g) Enter this set of arrays into the ANN. (h) Compute the performance indices. (i) Based on the indices, compare the forecasting with the results of step (a). (j) Repeat from step (b) until the last selected mother-wavelet is reached. In this paper, three statistical indices were used to evaluate the model performance (Appendix): the coefficient of determination (R2), the root mean square error (RMSE) and the Nash–Sutcliffe coefficient (NASH). 498 P.K.M.M. Freire, C.A.G. Santos and G.B.L. Silva / Applied Soft Computing Journal 80 (2019) 494–505 Fig. 3. Scheme of the ANN processing flow forecast. Fig. 4. ANN architecture. Fig. 5. Location of the Sobradinho Reservoir in the São Francisco River basin, Brazil. The linear combinations of the four sequential approximations mentioned in step (f) were proposed for the first time by Santos and Silva [32], and the advantages of such a procedure have recently been discussed by Honorato et al. [39]. The performance indices mentioned in step (h) can provide different types of information about the predictive capabilities of the model. Ad- ditionally, NASH and R 2 (relative errors) can provide a useful comparison with other studies because they do not depend on the scale of the data used [40]. The coefficient of determination (R2) measures the degree of correlation among the observed and predicted values. R2 is a measure of the ability of the model to develop a relationship among input and output variables [26]. The RMSE provides an idea of the absolute accuracy of the models and sizes the goodness of the fit related to high flow values, while P.K.M.M. Freire, C.A.G. Santos and G.B.L. Silva / Applied Soft Computing Journal 80 (2019) 494–505 499 Fig. 6. Structure of the proposed hybrid system. Fig. 7. Time series of the seven-day forecast using the raw data: (a) calibration and (b) validation. the NASH indicates how well the model predicts values away from the mean estimator [41,42]. Then, the optimal models will be those with NASH and R2 values close to unity and low RMSE values. The optimal level of decomposition (nopt) was set to 10 be- cause it is the mode of the possible maximum levels (lmax) for each of the 54 wavelet decompositions, which was determined according to the time series size (lx = 29 220) and filter length 500 P.K.M.M. Freire, C.A.G. Santos and G.B.L. Silva / Applied Soft Computing Journal 80 (2019) 494–505 Fig. 8. Time series of observed streamflow (Qo) and streamflow forecasted with the proposed ANN hybrid model using discrete Meyer mother-wavelet with A3 (Qc ) for (a) calibration and (b) validation. (lw) of the selected mother-wavelet. The maximum level is the last level for which at least one coefficient is correct [43]: lmax = log ( lx lw −1 ) log(2) (2) 3. Results and discussion For convenience and to remove the seasonality in the mean and variance, the inputs and target (raw data) were linearly standardized and scaled so that they fell within the range [−1, 1] before starting the training process to improve the efficiency of the ANN [35]. Networks trained on standardized data yield better results in general [44]. The data were divided into 70% training, 15% validation and 15% testing sets. During the training process, the validation error was monitored, and the errors in the validation and training sets usually decreased during the initial training. As soon as the data become overfitted by the ANN, the validation error increases for a number of iterations, and the training ceases. The training stops when the error in the validation data set begins to rise, which ensures that the ANN does not overfit the training data and subsequently fails to generalize the unseen test data set [45]. Fig. 7a shows the seven-day forecasted time series using the stand-alone ANN during training, which uses data from 1 January 1931 to 31 December 2007, whereas Fig. 7b shows the forecasted time series during the validation period, which uses data from 1 January 2008 to 31 December 2010. The forecasting results during the training and validation periods provided RMSE values equal to 0.3998 and 456.7712 m3/s, respectively. Fig. 7b shows that there was a lag of seven days between the observed and predicted values. This phenomenon has been previously reported by other authors (e.g., [13,27]) for forecasts using ANN. The streamflow time series was transformed using 54 mother- wavelets to decompose the series into approximations and de- tails, which were then divided into two periods: (a) calibration and (b) validation. Finally, single approximations and their com- binations (34 types of linear combinations) were used to perform seven-day forecasting. Thus, the total number of wavelet-ANN hybrid models was 1836, i.e., 54 × 34. Table 2 shows the best combination on the basis of perfor- mance indices obtained in each subfamily, which can be com- pared with the results obtained when the stand-alone ANN was used to perform the forecasting (Fig. 7). Table 2 indicates that all the performance indices during the validation period surpassed those calculated with the raw signal. Regardless of the wavelet family used, it was always possible to identify a combination of approximations that improved the ANN performance when compared to the use of the raw signal as input information. Among all the cases tested, the discrete Meyer mother-wavelet using the approximation A3 presented the best results. Fig. 8 shows the time series during the calibration and validation of the best wavelet-ANN model (discrete Meyer mother-wavelet). The lag detected in Fig. 7b was eliminated (Fig. 8b), which can be considered a notable contribution for using wavelet transforms coupled with ANNs. In addition, the proposed hybrid wavelet- ANN system eliminated the minor fluctuations present in the forecasted streamflow time series (Fig. 7b). However, the individual use of the A3 approximation was not the alternative that led to the best ANN performance for most of the tested wavelet families, and the combination of this with P.K.M.M. Freire, C.A.G. Santos and G.B.L. Silva / Applied Soft Computing Journal 80 (2019) 494–505 501 Fig. 9. Variation in RMSE of the proposed hybrid system in relation to the stand-alone ANN. other approximations was necessary. Additionally, the best com- binations varied among the several wavelet families evaluated. These results suggest that in studies that utilize hybrid wavelet- ANN modeling for streamflow forecasting purposes, it may be important to test different configurations of wavelet families and combinations of approximations to maximize the performance of the proposed model. Another aspect that can be highlighted from Table 2 is that even for the rbio2.2, rbio3.1 and rbio3.3 families, where the best results were also obtained with the A3 approximation, the RMSE values increased significantly (389.4784 m3/s, 295.0298 m 3/s and 428.0309 m 3/s, respectively) in relation to that obtained for dmey (96.4523 m 3/s), indicating degradation in the ANN fore- casts. These results indicate that the equivalent approximations obtained from different wavelets may not preserve the informa- tion needed to maintain the same level of ANN performance. Fig. 9 shows the variation in the RMSE of the proposed hybrid system in relation to the stand-alone ANN, using approxima- tions or approximation combinations of the best subfamilies, i.e., ∆RMSE = (RMSEhybrid – RSMEstand−alone)/RSMEstand−alone. For example, 10% means that the RMSE computed for the hybrid ANN is 10% greater than the RMSE computed for the stand- alone ANN and vice versa. Then, the negative values in Fig. 9 indicate that the hybrid ANN performed better than the stand- alone ANN because the RMSE value decreased. Even if only one approximation is used, the RMSE decreases (this can be seen up to the approximation A5) and the lowest observed values for the best approximations of each wavelet subfamily are concentrated in a narrow region between A3 and A4. Thereafter, the RMSE increases considerably. Similar behavior can be observed in relation to the combinations among the approximations. In most cases, the linear combinations of A1 + A2 to A4 + A5 + A6 + A7 were below 0%, i.e., they showed improvement, and the minimum values among the evaluated combinations were concentrated in the range between A3 + A4 and A3 + A4 + A5 (Fig. 9). Individually, each approximation can be interpreted in terms of the sum between the raw signal and successive levels of detail. For example, as explained before, Q = A1 + D1, then A1 = Q – D1; A2 = Q – D1 – D2 and so on. Therefore, the higher the level of approximation is, the greater the amount of details (high- frequency components) that are removed from the original signal. Thus, the results shown in Fig. 9 indicate that the subtraction of the raw signal details D1, D2 and D3 was essential to improve the ANN performance in the hybrid model because the use of A3 and A4 approximations (which do not contain these high-frequency components) led to the best results. The same condition applies to the combinations A3 + A4 and A3 + A4 + A5. This result means that the information contained in these high-frequency components was not essential to maximizing the performance of the proposed ANN. This finding explains, to a certain extent, the following analyses. The use of the transformed signal improved the forecasting by 78.88% (Table 3), reducing the RMSE from 456.7712 m3/s to 96.4523 m 3/s when the discrete Meyer mother-wavelet was used. This approximation combination used only the approximation A3 to compose the best wavelet-ANN hybrid system. Table 3 shows the percentages of the increases and decreases of the performance indices from using the wavelet-ANN hy- brid system compared to the stand-alone ANN. However, this 502 P.K.M.M. Freire, C.A.G. Santos and G.B.L. Silva / Applied Soft Computing Journal 80 (2019) 494–505 Table 2 The best approximation combinations for each mother-wavelet for seven-day forecasting. Wavelet family Best combination Calibration Validation R 2 NASH RMSE (m3/s) R2 NASH RMSE (m 3/s) None – 0.9708 0.9423 0.3998 0.9481 0.8971 456.7712 Haar A2 + A3 + A4 + A5 0.9751 0.9508 1.1495 0.9613 0.9235 393.7655 db1 A2 + A3 + A4 + A5 0.9724 0.9455 1.7519 0.9578 0.9169 410.2993 db2 A3 + A4 + A5 + A6 0.9823 0.9649 0.1892 0.9675 0.9358 360.6302 db3 A3 + A4 + A5 0.9890 0.9780 0.0833 0.9798 0.9599 284.9608 db4 A3 + A4 + A5 0.9923 0.9847 1.3030 0.9857 0.9715 240.4986 db5 A3 + A4 + A5 0.9940 0.9881 0.1974 0.9874 0.9749 225.6829 db6 A3 + A4 + A5 0.9950 0.9900 0.5719 0.9921 0.9842 178.8269 db7 A3 + A4 + A5 0.9956 0.9912 3.5325 0.9909 0.9818 192.0480 db8 A3 + A4 + A5 0.9962 0.9924 0.7624 0.9933 0.9867 164.4941 db9 A3 + A4 + A5 0.9964 0.9928 0.5920 0.9939 0.9879 156.7377 db10 A3 + A4 0.9972 0.9945 0.0858 0.9942 0.9885 152.6617 sym2 A3 + A4 + A5 + A6 0.9826 0.9655 3.0188 0.9679 0.9367 358.2598 sym3 A3 + A4 + A5 0.9893 0.9788 4.2927 0.9800 0.9600 284.6188 sym4 A3 + A4 + A5 0.9922 0.9844 0.6678 0.9848 0.9697 247.7887 sym5 A3 + A4 + A5 0.9939 0.9878 0.3652 0.9879 0.9758 221.3999 sym6 A3 + A4 + A5 0.9952 0.9904 0.5385 0.9909 0.9818 192.0562 sym7 A3 + A4 + A5 0.9955 0.9910 3.9426 0.9915 0.9831 185.1047 sym8 A3 + A4 + A5 0.9962 0.9925 0.2342 0.9938 0.9875 158.9116 coif1 A3 + A4 + A5 + A6 0.9837 0.9677 1.0244 0.9710 0.9425 341.3648 coif2 A3 + A4 + A5 0.9926 0.9852 1.2201 0.9875 0.9750 225.1616 coif3 A3 + A4 + A5 0.9949 0.9898 0.9467 0.9899 0.9798 202.1880 coif4 A3 + A4 0.9964 0.9929 0.3717 0.9938 0.9876 158.6908 coif5 A3 + A4 0.9973 0.9947 1.4683 0.9950 0.9900 142.7231 bior1.1 A2 + A3 + A4 + A5 0.9734 0.9474 1.2138 0.9594 0.9194 404.2452 bior1.3 A2 + A3 + A4 + A5 0.9729 0.9465 0.9383 0.9551 0.9097 427.9314 bior1.5 A2 + A3 + A4 + A5 0.9709 0.9425 5.4841 0.9513 0.9030 443.3382 bior2.2 A3 + A4 + A5 + A6 0.9895 0.9792 0.3226 0.9794 0.9589 288.6358 bior2.4 A3 + A4 + A5 0.9904 0.9809 2.1279 0.9825 0.9652 265.6507 bior2.6 A3 + A4 + A5 0.9907 0.9816 0.6203 0.9847 0.9693 249.3796 bior2.8 A3 + A4 + A5 0.9891 0.9783 6.8227 0.9829 0.9660 262.5683 bior3.1 A3 + A4 0.9946 0.9893 0.7654 0.9885 0.9767 217.1830 bior3.3 A3 + A4 0.9951 0.9903 3.5291 0.9915 0.9830 185.6542 bior3.5 A3 + A4 + A5 0.9952 0.9904 1.1147 0.9910 0.9821 190.5892 bior3.7 A3 + A4 + A5 0.9955 0.9911 1.6250 0.9904 0.9809 196.7319 bior3.9 A3 + A4 + A5 0.9957 0.9915 0.0383 0.9920 0.9841 179.5453 bior4.4 A3 + A4 + A5 0.9942 0.9885 3.0239 0.9888 0.9777 212.8125 bior5.5 A3 + A4 + A5 0.9957 0.9914 2.5324 0.9921 0.9842 179.0416 bior6.8 A3 + A4 + A5 0.9961 0.9922 0.3740 0.9932 0.9865 165.5155 rbio1.1 A2 + A3 + A4 + A5 0.9746 0.9498 1.8107 0.9599 0.9206 401.0587 rbio1.3 A2 + A3 + A4 0.9908 0.9816 1.4124 0.9826 0.9648 267.0743 rbio1.5 A3 + A4 + A5 0.9946 0.9891 2.8243 0.9896 0.9791 205.8419 rbio2.2 A3 0.9760 0.9525 4.9000 0.9621 0.9252 389.4784 rbio2.4 A3 + A4 + A5 + A6 0.9865 0.9732 0.1832 0.9770 0.9542 304.8252 rbio2.6 A3 + A4 + A5 0.9928 0.9856 1.0053 0.9875 0.9750 224.9374 rbio2.8 A3 + A4 + A5 0.9950 0.9900 0.1133 0.9913 0.9826 187.8561 rbio3.1 A3 0.9864 0.9729 2.2514 0.9785 0.9571 295.0298 rbio3.3 A3 0.9731 0.9470 1.9948 0.9554 0.9096 428.0309 rbio3.5 A2 + A3 + A4 + A5 0.9826 0.9655 4.0960 0.9663 0.9323 370.4298 rbio3.7 A3 + A4 + A5 + A6 0.9888 0.9777 2.6436 0.9786 0.9577 292.9619 rbio3.9 A3 + A4 + A5 0.9936 0.9873 0.6015 0.9881 0.9762 219.4284 rbio4.4 A3 + A4 + A5 0.9891 0.9783 2.6875 0.9802 0.9606 282.4350 rbio5.5 A3 + A4 + A5 0.9908 0.9817 1.5963 0.9838 0.9677 255.9963 rbio6.8 A3 + A4 + A5 0.9952 0.9905 1.1344 0.9918 0.9837 181.5696 dmey A3 0.9986 0.9971 0.1357 0.9977 0.9954 96.4523 was done for only the best approximation combinations of each wavelet subfamily. In the R 2 and NASH columns, a positive value indicates the improvement to these indices because the best value is close to 1.0. In the RMSE column, the negative sign indicates improvement in this index because the best value is close to 0.0. The best approximation is A3, which is present in all the best combinations of the subfamilies, followed by A4. The worst approximations were A7, A8, A9 and A10. The predictions using A6, A7, A8, A9, A10 and their combinations, i.e., A6 + A7, A6 + A7 + A8, A6 + A7 + A8 + A9, A7 + A8, A7 + A8 + A9, A7 + A8 + A9 + A10, A8 + A9, A8 + A9 + A10, and A9 + A10, showed no improvement in the forecasting results according to the RMSE of the validation period when compared to the ANN system with the raw data. However, the combination A2 + A3 + A4 + A5 was successful in 52 subfamilies (out of the 54 total subfamilies), the combinations A1 + A2 + A3, A1 + A2 + A3 + A4 and A2 + A3 in 51 subfamilies and A1 and A2 + A3 + A4 in 50 subfamilies. The best subfamilies were Meyer discrete (dmey), Daubechies (db8), and coiflets (coif4 and coif5), as they were successful in 17 combinations out of the total of 34, while the biorthogonal (bior1.5 and bior1.3) subfamilies had the worst results with only one and two combinations, respectively. These combinations were A2 + A3 + A4 + A5 for the biorthogonal bior1.5 and A1 + A2 + A3 + A4 and A2 + A3 + A4 + A5 for the biorthogonal bior1.3. P.K.M.M. Freire, C.A.G. Santos and G.B.L. Silva / Applied Soft Computing Journal 80 (2019) 494–505 503 Table 3 Increase or decrease rates of the forecasting performance using the wavelet-ANN hybrid system compared to the stand-alone ANN. Wavelet family Best combination Percentage of improvement (%) Calibration Validation R2 NASH RMSE R 2 NASH RMSE Haar A2 + A3 + A4 + A5 0.44 0.89 187.53 1.39 2.95 −13.79 db1 A2 + A3 + A4 + A5 0.17 0.34 338.2 1.02 2.22 −10.17 db2 A3 + A4 + A5 + A6 1.19 2.40 −52.68 2.04 4.32 −21.05 db3 A3 + A4 + A5 1.87 3.79 −79.17 3.34 7.01 −37.61 db4 A3 + A4 + A5 2.22 4.49 225.92 3.97 8.29 −47.35 db5 A3 + A4 + A5 2.39 4.85 −50.63 4.15 8.67 −50.59 db6 A3 + A4 + A5 2.49 5.05 43.05 4.64 9.72 −60.85 db7 A3 + A4 + A5 2.56 5.19 783.58 4.51 9.45 −57.96 db8 A3 + A4 + A5 2.62 5.31 90.69 4.77 9.99 −63.99 db9 A3 + A4 + A5 2.64 5.36 48.08 4.83 10.12 −65.69 db10 A3 + A4 2.73 5.53 −78.54 4.87 10.19 −66.58 sym2 A3 + A4 + A5 + A6 1.22 2.46 655.07 2.09 4.42 −21.57 sym3 A3 + A4 + A5 1.91 3.86 973.71 3.37 7.02 −37.69 sym4 A3 + A4 + A5 2.21 4.46 67.03 3.87 8.10 −45.75 sym5 A3 + A4 + A5 2.38 4.83 −8.66 4.20 8.78 −51.53 sym6 A3 + A4 + A5 2.52 5.10 34.70 4.51 9.45 −57.95 sym7 A3 + A4 + A5 2.55 5.16 886.14 4.58 9.59 −59.48 sym8 A3 + A4 + A5 2.62 5.32 −41.42 4.82 10.09 −65.21 coif1 A3 + A4 + A5 + A6 1.33 2.69 156.23 2.41 5.07 −25.27 coif2 A3 + A4 + A5 2.25 4.55 205.17 4.15 8.69 −50.71 coif3 A3 + A4 + A5 2.48 5.03 136.80 4.41 9.23 −55.74 coif4 A3 + A4 2.64 5.36 −7.02 4.82 10.09 −65.26 coif5 A3 + A4 2.74 5.55 267.26 4.94 10.35 −68.75 bior1.1 A2 + A3 + A4 + A5 0.27 0.53 203.59 1.19 2.49 −11.50 bior1.3 A2 + A3 + A4 + A5 0.22 0.44 134.69 0.74 1.40 −6.31 bior1.5 A2 + A3 + A4 + A5 0.01 0.02 1271.72 0.33 0.66 −2.94 bior2.2 A3 + A4 + A5 + A6 1.93 3.91 −19.31 3.30 6.89 −36.81 bior2.4 A3 + A4 + A5 2.03 4.10 432.25 3.63 7.59 −41.84 bior2.6 A3 + A4 + A5 2.06 4.16 55.16 3.86 8.05 −45.40 bior2.8 A3 + A4 + A5 1.89 3.82 1606.53 3.68 7.68 −42.52 bior3.1 A3 + A4 2.46 4.98 91.44 4.26 8.88 −52.45 bior3.3 A3 + A4 2.51 5.09 782.71 4.58 9.58 −59.36 bior3.5 A3 + A4 + A5 2.52 5.11 178.80 4.53 9.48 −58.27 bior3.7 A3 + A4 + A5 2.55 5.17 306.44 4.47 9.35 −56.93 bior3.9 A3 + A4 + A5 2.57 5.22 −90.42 4.63 9.70 −60.69 bior4.4 A3 + A4 + A5 2.42 4.90 656.36 4.29 8.98 −53.41 bior5.5 A3 + A4 + A5 2.57 5.20 533.42 4.64 9.71 −60.80 bior6.8 A3 + A4 + A5 2.61 5.29 −6.46 4.76 9.97 −63.76 rbio1.1 A2 + A3 + A4 + A5 0.39 0.80 352.91 1.24 2.63 −12.20 rbio1.3 A2 + A3 + A4 2.06 4.16 253.27 3.64 7.55 −41.53 rbio1.5 A3 + A4 + A5 2.45 4.97 606.44 4.37 9.14 −54.94 rbio2.2 A3 0.54 1.08 1125.62 1.48 3.13 −14.73 rbio2.4 A3 + A4 + A5 + A6 1.62 3.28 −54.17 3.04 6.36 −33.27 rbio2.6 A3 + A4 + A5 2.27 4.59 151.44 4.15 8.69 −50.75 rbio2.8 A3 + A4 + A5 2.50 5.06 −71.66 4.55 9.53 −58.87 rbio3.1 A3 1.61 3.24 463.13 3.20 6.69 −35.41 rbio3.3 A3 0.25 0.50 398.95 0.77 1.40 −6.29 rbio3.5 A2 + A3 + A4 + A5 1.22 2.46 924.51 1.92 3.93 −18.90 rbio3.7 A3 + A4 + A5 + A6 1.86 3.75 561.22 3.22 6.75 −35.86 rbio3.9 A3 + A4 + A5 2.35 4.77 50.44 4.22 8.83 −51.96 rbio4.4 A3 + A4 + A5 1.89 3.82 572.22 3.39 7.09 −38.17 rbio5.5 A3 + A4 + A5 2.07 4.18 299.27 3.76 7.87 −43.96 rbio6.8 A3 + A4 + A5 2.52 5.11 183.74 4.61 9.66 −60.25 dmey A3 2.86 5.82 −66.05 5.23 10.96 −78.88 4. Conclusions This study assessed the performance of a stand-alone ANN compared to a wavelet-ANN hybrid system for short-term stream- flow forecasting. The use of a wavelet transform for the elimina- tion of the signal noise was a substantial factor in obtaining the results, as this smoothing of the original time series improved the forecast accuracy. The hybrid systems presented significantly better results than the stand-alone ANN model. The best results were obtained using the approximations or their combinations and occurred between levels 2 and 6. The best wavelet order was typically found in the highest subfamilies because they contained a more detailed format (for example, see db8 and db10 in Fig. 2). The best wavelet-ANN hybrid system was composed of the discrete Meyer mother-wavelet (dmey) due to its symmetrical and detailed format (compare dmey with db2 in Fig. 2). Thus, this mother-wavelet was a suitable method to represent the studied streamflow time series. The approxima- tion A3 presented the best results, i.e., the elimination of details D1, D2 and D3 was sufficient to improve the forecasting. The wavelet-ANN hybrid system produced an R 2 = 0.9977, a NASH = 0.9954 and an RMSE = 96.4523 m 3/s, whereas the forecasting using the stand-alone ANN (raw data) gave the following results: R2 = 0.9481; NASH = 0.8971 and RMSE = 456.7712 m3/s. The RMSE decreased by almost 80%, and the coefficients R2 and NASH increased by more than 5% and 10%, respectively, compared to the 504 P.K.M.M. Freire, C.A.G. Santos and G.B.L. Silva / Applied Soft Computing Journal 80 (2019) 494–505 forecasting using the raw data as inputs for the ANN. In addition, the forecasting using the wavelet-ANN hybrid systems could also eliminate the lags and fluctuations detected in the forecasting using the stand-alone ANN. Acknowledgments The authors thank the ONS for providing the observed daily streamflow data and Brazil’s National Council for Scientific and Technological Development (CNPq) for financial support (Grant No. 304213/2017-9). This study was also financed in part by the Brazilian Agency for the Improvement of Higher Education (Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - CAPES) – Fund Code 001, and Universidade Federal da Paraíba, Brazil. Conflict of interest No author associated with this paper has disclosed any po- tential or pertinent conflicts which may be perceived to have impending conflict with this work. For full disclosure statements refer to https://doi.org/10.1016/j.asoc.2019.04.024. Appendix A.1. Coefficient of determination (R 2) This coefficient determines the variability of a projected num- ber around the true value. The optimal value of R2 is 1. R 2 = (∑N t=1 ( Qot − Qo) (Qct − Qc))2 ∑N t=1 ( Qot − Qo)2 ∑N t=1 ( Qct − Qc)2 (3) where Q o is the mean observed streamflow, and Q c is the mean calculated streamflow. A.2. Root mean square error (RMSE) This index is the square root of the mean square error (RMSE), whose optimal value is 0. RMSE =    √ 1 N N∑ t=1 ( Qct − Qot )2 (4) A.3. Nash–Sutcliffe coefficient (NASH) This coefficient is considered one of the most important sta- tistical criteria for evaluating the accuracy of hydrological models and can range from −∞ to 1. The optimal value is 1. NASH = 1 − ∑ (Qo − Qc) 2 ∑ ( Qo − Qo)2 (5) Thus, the best hybrid system wavelet-ANN is the one that shows a low RMSE and a NASH and R2 close to 1. References [1] D.N. Kumar, K.S. Raju, T. Sathish, River flow forecasting using recurrent neural network, Water Resour. Manag. 18 (2) (2004). [2] ONS, Operador Nacional do Sistema Elétrico, 2012. Available in www.ons. org.br. [3] S.J. Hadi, M. Tombul, Forecasting daily streamflow for basins with differ- ent physical characteristics through data-driven methods, Water Resour. Manage. (2018) http://dx.doi.org/10.1007/s11269-018-1998-1. [4] H. Tongal, M.J. Booij, A comparison of nonlinear stochastic self-exciting threshold autoregressive and chaotic k-nearest neighbour models in daily streamflow forecasting, Water Resour. Manage. 30 (2016) 1515–1531, http://dx.doi.org/10.1007/s11269-016-1237-6. [5] M.J.S. Valença, G.C. Vasconcelos, Estudo comparativo dos modelos Box- Jenkins, redes neurais e regressão múltipla na previsão de vazões. Simpósio Brasileiro de Redes Neurais, 4, Goiânia. 1997 55–57. [6] M. Campolo, P. Andreussi, A. Soldati, River flood forecasting with a neural network model, Water Resour. Res. 35 (4) (1999) 1191–1197, http://dx. doi.org/10.1029/1998WR900086. [7] N.T. Danh, H.N. Phien, A. Gupta, Neural network models for river flow forecasting, Water SA 25 (1) (1999). [8] N. Lauzon, J. Rousselle, S. Birikundavyi, H.T. Trung, Real-time daily flow forecasting using black-box models, diffusion processes, and neural net- works, Canadian J. Civil Eng. 27 (4) (2000) 671–682, http://dx.doi.org/10. 1139/cjce-27-4-671. [9] T. Ludermir, M. Valença, Monthly streamflow forecasting using an neu- ral fuzzy network model, in: International Join Conference on Neural Networks. 2000, Italy. Anais Do International Join Conference on Neural Networks, Montréal, Canada, 2000, http://dx.doi.org/10.1109/SBRN.2000. 889724. [10] T. Ludermir, M. Valença, Neural networks vs. PARMA modelling: Case stud- ies of river flow prediction, in: Brazilian Symposium on Neural Networks, 6, Rio de Janeiro, 2000b, pp. 113–116. [11] B. Sivakumar, A.W. Jayawerdana, T.M.K.G. Fernando, River flow forecast- ing: use of phase-space reconstruction and artificial neural networks approaches, J. Hydrol. 265 (1–4) (2002) 225–245, http://dx.doi.org/10.1016/ S0022-1694(02)00112-9. [12] H.K. Cigizoglu, Estimation, forecasting and extrapolation of flow data by artificial neural networks, Hydrological Sci. J. 48 (3) (2003) 349–361, http://dx.doi.org/10.1623/hysj.48.3.349.45288. [13] H.K. Cigizoglu, Ö. Kisi, Flow prediction by three back propagation tech- niques using k-fold partitioning of neural network training data, Nordic Hydrology 36 (1) (2005) 1–16. [14] S.A. Akrami, A. El-shafie, M. Naseri, C.A.G. Santos, Rainfall data analyzing using moving average (MA) model and wavelet multi-resolution intelligent model for noise evaluation to improve the forecasting accuracy, Neural Comp. Appl. 25 (7/8) (2014) 1853–1861, http://dx.doi.org/10.1007/s00521- 014-1675-0. [15] C.L. Wu, K.W. Chau, Y.S. Li, Methods to improve neural network performance in daily flows prediction, J. Hydrol. 372 (2009) 80–93. [16] W. Wang, J. Ding, Wavelet network model and its application to the prediction of hydrology, Nat. Sci. 1 (1) (2003) 67–71. [17] B. Cannas, A. Fanni, L. See, G. Sias, Data preprocessing for river flow fore- casting using neural networks: Wavelet transforms and data partitioning, Phys. Chem. Earth 31 (18) (2006) 1164–1171, http://dx.doi.org/10.1016/j. pce.2006.03.020. [18] V. Nourani, M.T. Alami, M.H. Aminfar, A combined neural-wavelet model for prediction of ligvanchai watershed precipitation, Eng. Appl. Artif. Intell. 22 (3) (2009) 466–472. [19] I.M. Johnstone, B.W. Silverman, Wavelet threshold estimators for data with correlated noise, J. Royal Statist. Soc. Ser. B 59 (2) (1997) 319–351. [20] S. Dahdouh, M. Dubois, E. Frenoux, A. Osorio, A 1D wavelet filtering for ultrasound images despeckling, Proc. SPIE 7629 (2010) 762907, http: //dx.doi.org/10.1117/12.844388. [21] J.L. San Emeterio, M.A. Rodriguez-Hernandez, Wavelet denoising of ultra- sonic A-scans for detection of weak signals, in: Proc. 19th International Conference on Systems, Signals and Image Processing, IWSSIP, 2012, pp. 48–51. [22] B. Cannas, A. Fanni, G. Sias, S. Tronci, M.K. Zedda, River flow forecasting using neural networks and wavelet analysis, Geophys. Res. Abst. 7 (2005) 08651. [23] Ö. Kisi, Wavelet regression model for short-term streamflow forecasting, J. Hydrol. 389 (3–4) (2010) 344–353. [24] R. Maheswaran, R. Khosa, Wavelet-Volterra coupled model for monthly stream flow forecasting, J. Hydrol. 450–451 (2012) 320–335. [25] Ö. Kisi, Neural networks and wavelet conjunction model for intermittent stream-flow forecasting, J. Hydrol. Eng. 14 (8) (2009) 773–782. [26] J. Adamowski, K. Sun, Development of a coupled wavelet transform and neural network method for flow forecasting of non-perennial rivers in semi-arid watersheds, J. Hydrol. 390 (1–2) (2010) 85–91. [27] N. Pramanik, R.K. Panda, A. Singh, Daily river flow forecasting using wavelet ANN hybrid models, J. Hydroinform. 13 (1) (2010) 49–63. [28] B. Krishna, Y.R. Satyaji Rao, P.C. Nayak, Time series modeling of river flow using wavelet neural networks, J. Water Resour. Protec. 3 (2011) 50–59. [29] B. Krishna, Comparison of wavelet based ANN and regression models for reservoir inflow forecasting, J. Hydrol. Eng. (2013) http://dx.doi.org/10. 1061/(ASCE)HE.1943-5584.0000892. [30] Y. Seo, S. Kim, O. Kisi, V.P. Singh, Daily water level forecasting using wavelet decomposition and artificial intelligence techniques, J. Hydrol. 520 (224–243) (2015) http://dx.doi.org/10.1016/j.jhydrol.2014.11.050. P.K.M.M. Freire, C.A.G. Santos and G.B.L. Silva / Applied Soft Computing Journal 80 (2019) 494–505 505 [31] C.A.G. Santos, P.K.M.M. Freire, G.B.L. Silva, R.M. Silva, Discrete wavelet transform coupled with ANN for daily discharge forecasting into Três Marias reservoir, Proc. IAHS 364 (2014) 100–105, http://dx.doi.org/10. 5194/piahs-364-100-2014. [32] C.A.G. Santos, G.B.L. Silva, Daily streamflow forecasting using a wavelet transform and artificial neural network hybrid models, Hydrol. Sci. J. 59 (2) (2014) 312–324, http://dx.doi.org/10.1080/02626667.2013.800944. [33] V. Nourani, A.H. Baghanam, J. Adamowski, O. Kisi, Applications of hybrid wavelet–artificial intelligence models in hydrology: A review, J. Hydrology. 514 (2014) 358–377, http://dx.doi.org/10.1016/j.jhydrol.201403057. [34] S. Mallat, A theory for multiresolution signal decomposition: the wavelet representation, IEEE Trans. Pattern Anal. Mach. Intell. 11 (7) (1989) 674–693. [35] H. Demuth, M. Beale, Neural network toolbox: for use with matlab, in: The MathWorks, 469, Inc. Natick, USA, 2005. [36] S. Haykin, Neural Networks: A Comprehensive Foundation, second ed., Prentice Hall, Upper Saddle River, USA, 1999. [37] Ö. Kisi, E. Uncuoglu, Comparison of three back-propagation training al- gorithms for two cases studies, Indian J. Eng. Mater. Sci. 12 (2005) 434–442. [38] B. Sharma, K. Venugopalan, Comparison of neural network training func- tions for hematoma classification in brain CT images, J. Comput. Eng. 16 (1) (2014) 31–35. [39] A.G.S.M. Honorato, G.B.L. Silva, C.A.G. Santos, Monthly streamflow fore- casting using neuro-wavelet technique and input analysis, Hydrol. Sci. J. 63 (15–16) (2018) http://dx.doi.org/10.1080/02626667.2018.1552788. [40] N. Karunanithi, W.J. Grenney, D. Whitley, Bovee. K., Neural networks for river flow prediction, J. Comput. Civ. Eng. 8 (2) (1994) 201–220. [41] C.W. Dawson, R.J. Abrahart, A.Y. Shamseldin, R.L. Wilby, Flood estimation at ungauged sites using artificial neural networks, J. Hydrol. 319 (1–4) (2006) 391–409, http://dx.doi.org/10.1016/j.jhydrol.2005.07.032. [42] O. Kisi, Neural networks and wavelet conjunction model for intermittent streamflow forecasting, J. Hydrol. Eng. 14 (8) (2009) http://dx.doi.org/10. 1061/(ASCE)HE.1943-5584.0000053. [43] M. Misiti, Y. Misiti, G. Oppenheim, J.M. Poggi, Wavelet Toolbox User’S Guide - for Use with MATLAB, third ed., Massachusets (USA): The MathWorks, Inc., 2006. [44] M. Shanker, M.Y. Hu, M.S. Hung, Effect of data standardization on neural network training, Omega 24 (4) (1996) 385–397, http://dx.doi.org/10.1016/ 0305-0483(96)00010-2. [45] A. Joorabchi, H. Zhang, M.M. Blumenstein, Application of artificial neural networks in flow discharge prediction for the Fitzroy River, Australia, J. Coastal Res. 50 (2007) 287–291.","libVersion":"0.0.0","langs":"","hash":"","size":0}